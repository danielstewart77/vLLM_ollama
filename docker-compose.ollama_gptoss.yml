# docker-compose.ollama.yml  (only vllm service shown)
services:
  vllm:
    image: vllm/vllm-openai:gptoss   # <-- gpt-ossâ€“ready build
    container_name: vllm
    runtime: nvidia
    command:
      - --model
      - openai/gpt-oss-20b
      - --host
      - 0.0.0.0
      - --port
      - "7000"
      # TP=1 is fine on a single A5000; keep TP=2 if you like spreading across both
      - --tensor-parallel-size
      - "1"
      # Let vLLM pick the right kernels/quantization for gpt-oss; no need to force dtype
      - --max-model-len
      - "131072"            # 128k
      - --max-num-seqs
      - "4"
      - --max-num-batched-tokens
      - "16384"
      - --enforce-eager
      - --trust-remote-code
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - NCCL_DEBUG=INFO
      - NCCL_IB_DISABLE=1
      # If shared memory congestion appears in logs, optionally:
      # - NCCL_SHM_DISABLE=1
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864
    volumes:
      - ./hf_cache:/root/.cache/huggingface
    ports:
      - "7000:7000"
    networks:
      - vllmnet
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:7000/health"]
      interval: 10s
      timeout: 3s
      retries: 10

  ollama-proxy:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-proxy
    environment:
      - VLLM_BASE_URL=http://vllm:7000
    depends_on:
      vllm:
        condition: service_healthy
    ports:
      - "3000:3000"
    volumes:
      - ./hf_cache:/root/.cache/huggingface
    networks:
      - vllmnet
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:3000/_debug"]
      interval: 10s
      timeout: 3s
      retries: 10

networks:
  vllmnet: