services:
  vllm:
    image: vllm/vllm-openai:gptoss
    container_name: vllm
    runtime: nvidia
    command:
      - --model
      - openai/gpt-oss-20b
      - --served-model-name
      - openai/gpt-osa-20b:latest
      - --host
      - 0.0.0.0
      - --port
      - "7000"
      - --tensor-parallel-size
      - "1"
    #  - --dtype
    #  - bfloat16
      - --gpu-memory-utilization
      - "0.70"
      - --max-model-len
      - "131072"
      - --max-num-seqs
      - "4"
      - --max-num-batched-tokens
      - "16384"
      - --enforce-eager
      - --trust-remote-code
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - NCCL_DEBUG=INFO
      - NCCL_IB_DISABLE=1
      # If shm still fills up, uncomment the next line:
      # - NCCL_SHM_DISABLE=1
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864
    volumes:
      - ./hf_cache:/root/.cache/huggingface
    ports:
      - "7000:7000"
    networks:
      - vllmnet
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:7000/health"]
      interval: 10s
      timeout: 3s
      retries: 10

  ollama-proxy:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-proxy
    environment:
      - VLLM_BASE_URL=http://vllm:7000
    depends_on:
      vllm:
        condition: service_healthy
    ports:
      - "3000:3000"
    volumes:
      - ./hf_cache:/root/.cache/huggingface
    networks:
      - vllmnet
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:3000/_debug"]
      interval: 10s
      timeout: 3s
      retries: 10

networks:
  vllmnet:
